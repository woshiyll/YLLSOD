    
The specific details of dataset construction are as follows:

    This dataset is derived from various publicly available SOD datasets and manually annotated data. It includes RGB, RGB-D (such as NLPR, SIP, SSD, STERE, ReD-Web, and NJUD), RGB-T (such as VT821, VT1000, and VT5000), VDT-2048, and VI-RGBT1500. A total of 2813 low-light images were selected. Specifically, 252 images were obtained from the RGB dataset, referred to as RGB-252. 385 images came from the RGB-D dataset, named RGBD-385. 621 images were sourced from the RGB-T dataset, known as RGBT-621. 766 images were taken from the VDT-2048 dataset, labeled as VDT-766. Lastly, 789 images were collected from the VI-RGBT1500 dataset, referred to as VI-789.

    The dataset is divided into a training set and a testing set. Out of the selected 2813 low-light images, 1986 images were allocated to the training set, which includes 200 images from RGBD-385, 465 images from RGBT-621, 152 images from RGB-252, 580 images from VDT-766, and 589 images from VI-789. The remaining 827 images were assigned to the testing set, consisting of 185 images from RGBD-385, 156 images from RGBT-621, 100 images from RGB-252, 186 images from VDT-766, and 200 images from VI-789.

    To further enrich the dataset and validate the generalization of SOD models, we created a test set called LL-450, which consists of 450 low-light images along with their corresponding saliency object masks. These low-light images were selected from various low-light datasets, including indoor and outdoor scenes, featuring keywords such as cups, plush toys, everyday objects, cars, bicycles, people, signs, and buildings. We employed a "majority voting" approach to identify the salient objects in each image and removed samples without salient regions. Saliency object annotations were performed at the pixel level using annotation software such as Adobe Photoshop. In many extremely dark scenes, salient objects may not be clearly visible. To ensure the accuracy of annotations, we used their corresponding normal-light images as references.